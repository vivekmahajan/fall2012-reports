%File: formatting-instruction.tex
\documentclass[letterpaper]{article}
\usepackage{aaai}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage{graphicx}
\frenchspacing
%\pdfinfo{
%/Title (Formatting Instructions for Authors Using LaTeX)
%/Subject (AAAI Publications)
%/Author (AAAI Press)}
\setcounter{secnumdepth}{1}  
 \begin{document}
% The file aaai.sty is the style file for AAAI Press 
% proceedings, working notes, and technical reports.
%
\title{Finding communities in graphs}
\author{Vivek Mahajan\\
vmahajan@sfu.ca\\
}
\maketitle
\section*{Abstract}
Detecting clusters or communities in large real-world graphs such as large
social or information networks is a problem of considerable interest. In
general, it is the organization of vertices in clusters, with many edges
joining vertices of the same cluster and comparatively few edges joining
vertices of different clusters.  In practice, one typically chooses an
objective function that captures the intuition as given by the definition above,
and then one applies approximation algorithms or heuristics to extract sets of
nodes which appears to be "good" with respect to that objective function.

In this survey, we will start with the applications of clustering in various
domains. We will describe various objective functions and will also discuss
some of the most cited community finding papers. 


\section{Introduction}
The origin of graph theory dates back to Euler's solution of the puzzle of
Konigsberg's bridges in 1736.  Since then a lot has been learned about graphs
and their mathematical properties. It is arguably the most widely adopted
representation which can be used to abstract lots of  real world problems. The
problems in Biology, sociology and information networks can be formulated as
graphs, and thus the graph analysis has become crucial to understand the
features of these systems. Recently, with faster processors and plenty of
computational resources, researchers can process and analyze huge amount of
data, which wasn't possible in earlier times.

Graphs representing real systems are more often not regular graphs. Some are
random graph with highly homogeneous degree distribution. Real networks are not
random graphs, as they display inhomogeneities, revealing a high level of order
and organization, in which high degree vertices can co-exist with low degree
vertices. Furthermore, the distribution of edges is not only globally, but also
locally inhomogeneous, with high number of edges within some group of vertices,
and low number of edges between these groups. This special feature of real
networks is called community structure or clustering. Communities, also called
clusters or modules, are groups of vertices which most likely share some common
properties. 

\begin{figure}
%\begin{minipage}[htb]{0.48\linewidth}
\includegraphics[scale=0.40]{pics/intro.eps}
\caption{A simple graph with three communities, enclosed by the dashed circles.}
\label{fig:bipart}
%\end{minipage}
\end{figure}

The communities exists in society, like families, working and friendship
circles, towns, nations etc.  The ubiquity of Internet has also resulted into
creation of virtual groups like online communities in social networks like
Facebook or LinkedIn. Communities also occur in many networked systems from
biology, computer science, economics, politics etc.  In protein-protein
interaction networks, communities are likely to group proteins having the same
functional property in the cell. In WWW the may correspond to groups of pages
dealing with same related topics, and so on.


In WWW, clustering users who have similar interests and who are geographically
near to each other may improve the performance of services provided on the
World Wide Web, where each cluster of clients could be served by a dedicated
mirror server. Clustering can be used  to generate recommendations on
e-commerce websites, e.g users with similar interests can be clustered and
could be given appropriate recommendation.  Clusters in large sparse graphs can
be used to create data structures in order to efficiently store the graph data
and to handle navigational queries, like path searches. It can also used in
case of Ad hoc networks, as they normally don't maintain a central routing
tables. Also, clustering can be used in case of caching of web pages as well.

The goal of detecting communities in graphs is to identify the clusters and,
more often, their hierarchical organization, by using the structure properties
of the graph. The problem itself is very old and have lots of proposed
algorithms in different disciplines.

The first analysis of community structure was carried out by Weiss et al. in
1957, who searched for group of workers in a government agency. Even earlier
work was done in finding clusters of people in small political bodies, based on
the similarity of their voting pattern.

Finding graph communities is also popular in computer science. In parallel
computing, it is quite common to find out the best way to allocate tasks to
processors so as to minimize the communications between them. This is also
called graph partitioning problem which will be discussed in the later sections.
In a seminal paper appeared in 2002, Girvan and Newman proposed a new algorithm
for finding the edges lying between communities and iteratively removing them
to find the clusters \cite{new2002}. After this paper, the field was revived and people
gave lots of sophisticated algorithms to tackle this problem.

In this survey, we try to cover some of the most popular algorithms in this
area. %We start with giving some examples of communities in Real-world networks.
In section 2, we will give definition of various objective functions used
nowadays. We will continue the discussion with explaining one of the most widely
used objective function in section 3. Followed by explanation of some
traditional approaches in section 5, 6, 7 and 8. We will then give a brief
overview of a more recent method in section 9 and then we will conclude in
section 10.
%\subsection{Communities in Real-World Networks}
\section{Objective Functions}
If you have clusters of a given graph as an output of a clustering algorithm,
we need to have some quantitative measure of how "good" they are. We call them
objective function which the algorithms try to optimize. The intuition behind
most of the objective functions is that a good cluster has many edges
internally and few edges pointing outside. In this section, we look at the
other objective functions that are based on this intuition.


In general, the two criterias contributes heavily to the formulation of
objective function. The first is the number of edges between the members of the
clusters, and second is the number of edges between the members of the cluster
and remainder of the network. So, based on whether an objective function is
using either one or both criteria in its formulation, we can divide the
objective functions into two categories\cite{leskovec2010}. The first group,
that we refer to as Multi-criterion scores, combines both criteria into a
single objective function; while the second group of objective functions uses
only  a single of the two criteria. We will give a quick overview of the
various definitions of the objective functions which falls under the two
categories.


\subsection{Multi-criterion scores}
Let $G(V,E)$ be an undirected graph with $n=|V|$ and $m=|E|$ edges. Let S be the set of nodes in the cluster, where $n_S$ is the number of nodes in S,
$n_S=|S|$; $m_S$ the number of edges in $S$,  $m_S=|\{(u,v):u\in S, v\notin S\}|;$ and $d(u)$ is the degree of node $u$.

We consider the following metrics $f(S)$ that capture the notion of a quality of the cluster. Lower value of score $f(S)$ signifies a more community-like set of 
nodes.
\begin{itemize}
\item \textbf{conductance}: $f(S) = \frac{c_S}{c m_S+c_S}$ measures the fraction of edges that points outside the
cluster \cite{good04,shi2000}
\item \textbf{Expansion}: $f(S) = \frac{c_S}{n_S}$ measures the number of edges per node that point outside the
cluster \cite{radi04}
\item \textbf{Internal density}: $f(S) = 1 - \frac{m_S}{n_S(n-n_S)}$ is the fraction of all possible edges leaving
the cluster \cite{radi04}
\item \textbf{Cut Ratio}: f(S) = $\frac{c_S}{n_S(n-n_S)}$ is the fraction of all possible edges leaving the cluster 
\item \textbf{Normalized Cut}:  f(S) = $\frac{c_S}{2m_S+c_S} + \frac{c_S}{2(m-m_S)+c_S}$ \cite{shi2000}
\end{itemize}

\subsection{Single-criterion scores}
As described earlier Single criterion scores are the scores which uses one of
the criterias to give a quantitative measure for the "goodness" of the
communities. One of the most widely used criteria is Modularity
\cite{newmod2006} which measure the number of within community edges, relative
to a null model of a random graph with the same degree distribution.

Here are few more single criterion scores
\begin{itemize}
\item \textbf{Modularity}: $\frac{1}{4m}(m_S-E(m_S))$, where $E(m_S)$ is the expected number of edges between
the nodes in set $S$ in a random graph with the same degree distribution.
\item \textbf{Modularity ratio}: $\frac{m_S}{E(m_S)}$ is alternative definition of the modularity, where we take 
the ratio of the number of edges between the nodes of $S$ and the expected number of such edges under the
null-model
\item \textbf{Volume}: $\sum_{u\in S}d(u)$ is sum of degrees of nodes in $S$
\item \textbf{Edges cut}:$c_S$ is the number of edges needed to be removed to disconnect nodes in $S$ from the 
rest of the network
\end{itemize}

\section{Modularity}
One of the most popular objective function is Modularity \cite{new2004}. It is
based on the intuition that a random graph is not expected to have a cluster
structure. So one could tell the quality of the clusters by comparing the edge
distribution inside the cluster with that of a random subgraph. This expected 
expected edge density is given by a \emph{null model}. It is the original graph
with some of its original structural properties but without any community structure.
Formally, modularity is
\begin{equation} Q = \frac{1}{2m}\sum_{ij}(A_{ij}-P_{ij})\delta(C_i,C_j),\end{equation}
where $A$ is the adjacency matrix, $m$ is the total number of edges of the
graph, and $P_{ij}$ is the expected number of edges between vertices $i$ and
$j$ in the null model. The $\delta$ is 1 if vertices $i$ and $j$ are in the
same community. The null model selection is subjective and lot of research has
been done on it. One simple model could be that the graph keeps the same number
of edges as the original graph, and that edges are placed with the same
probability between any pair of vertices.




\begin{figure}
%\begin{minipage}[htb]{0.48\linewidth}
\includegraphics[scale=0.25]{pics/bipartition.eps}
\caption{Bisection of graph such that the number of edges between the partitions are minimized. The dotted line
is the proposed solution.}
\label{fig:bipart}
%\end{minipage}
\end{figure}
\section{Traditional Approaches} In the following sections we will discuss some
of the traditional approaches towards finding communities and then we will move
on to discuss some of the more recent ones as well.  

\section{Graph
Partitioning} The problem of graph partitioning consists of dividing a graph
into $k$ groups of predefined size, such that the edges lying between the $k$
partitions are minimized. 

In the graph partitioning algorithms you need to specify the number of
partitions needed. Suppose, we don't specify it, then a trivial solution will
be to group all the vertices into one cluster such that number of edges lying
outside the partition is reduced to zero.  Also, specifying the size of the
partitions is important as without it, the size of partitions formed will be
highly unbalanced. Though we can avoid this problem by choosing a different
objective function.

Graph partitioning is quite ubiquitous in parallel computing, circuit
partitioning and layout, and in solving partial differential equations as well.
Most of the variations are NP-hard. As a result there are lots of heuristics
that does pretty good job. Most of the algorithms perform a bisection of the
graph. However, partitioning into more than two clusters can be achieved by
iterative Bi-sectioning. 

The \emph{Kernighan-Lin algorithm} \cite{kerni70} is one of the earliest
algorithms proposed and is still widely used as a last step of various graph
partitioning algorithm. The algorithm was discovered when they were working on
the problem of partitioning electronic circuits onto boards. The algorithm is
basically an optimization of an objective function $Q$, which represents the
difference between the number of edges inside the modules and the number of
edges lying between them.  The input to the algorithm is the initial partition
of the graph in two clusters of predefined size.  The initial partitions can be
randomly generated or can be the output of another graph partitioning
algorithm. Then, equal number of vertices are swapped between the two
partitions, so that $Q$ has the maximal increase. To avoid trapping in the
local maxima of $Q$, the algorithm includes some swaps that decrease the
function $Q$. After a series of swaps with positive and negative gains, the
partition with the largest value of Q is selected and used as starting point of
a new series of iterations.  The algorithm is known to converge after some
iterations and output depends highly on the initial partition.  So this graph
is mostly used to \emph{refine} the output of some other graph partitioning
algorithms. The run time complexity of it is $O(n^2 log\ n)$. But on the sparse
graph, it reduces to $O(n^2)$. 

Another widely used technique is the \emph{spectral bisection method}
\cite{barnes82}, which is based on the properties of the spectrum of the
Laplacian matrix. Spectral clustering will be discussed in detail later.  Just
a quick note here, that it is quite fast as it depends on computing the
eigenvectors of the Laplacian whose computation is relatively fast. The method
gives good partitions, which can be further improved by applying Kernighan-Lin
algorithm.


In general, the algorithms used for graph partitioning are not good for finding
communities, as it is necessary to provide the number of partitions and the
size of partitions as an input, which is quite unintuitive in case of finding
communities. Instead, we would except a community finding algorithm to give
that as an output.  



\begin{figure}
%\begin{minipage}[htb]{0.48\linewidth}
\includegraphics[scale=0.30]{pics/heiro.eps}
\caption{A dendogram, or hierarchical tree. Horizontal cut correspond to
partitions of the graph in communities} \label{fig:heiro}
%\end{minipage}
\end{figure}
\section{Hierarchical Clustering} In general, we don't know much about the
community structure of a graph. It is highly unlikely to know the number of
communities in a graph as well the size of it. In such cases, the graph
partitioning approaches can only be used on a hit and trial basis, where you
experiment with the input to the partitioning algorithm. On the other hand, the
graph may have a hierarchical structure, i.e it may display several levels of
grouping of the vertices, with small clusters included within larger clusters,
and so on. So in such cases a hierarchical algorithm, i.e an algorithm which
reveals hierarchical structure might be useful. 

In most of the hierarchical algorithms, the starting point is the definition of
a similarity measure between vertices. After a measure is chosen, one computes
the similarity for each pair of vertices.  This gives a new n*n matrix X, the
similarity matrix. Focus of the hierarchical clustering techniques is to find
groups of vertices with high similarity, and can be broadly classified into two
categories:

\subsection{Agglomerative Approach}
In this bottom-up approach, one starts from the vertices as separate
clusters(singletons) and ends up with the graph as a unique cluster. The
clusters with high mutual similarity are merged, so we need to define a measure
that estimates how similar clusters are, our of the similarity matrix $X$. In
single linkage clustering, the similarity between two groups is the minimum
element $x_{ij}$, with $i$ in one group and $j$ in the other. On the other
hand, in complete linkage clustering maximum element $x_{ij}$ is used. while in
average linkage clustering we need to compute the average of $x_ij$.
Figure \ref{fig:heiro} gives a good illustration of a hierarchical algorithm. The red
line in the figure denotes a stopping conditions which can be imposed based on
the number of vertices in partitions or other objective function. 

The main advantage of using a hierarchical clustering is that it does not
require a preliminary knowledge on the number and size of the clusters. On the
other hand, it does not provide any stopping criterion on its own. Also, the
results depends on the similarity criteria chosen. It also produces an
\emph{artificial} hierarchical tree, even if a graph doesn't have any hierarchy
in it. Also it is often the case that a node with only one neighbor forms a
separate cluster, which is not good. However, the main disadvantage is it does
not scale well.  Computing the similarity matrix is most expensive with best
case of $O(n^2)$ in case of single linkage, while $O(n^2 log\ n)$ in other.


\begin{figure}
\includegraphics[scale=0.30]{pics/edgeb.eps}
\caption{Edge betweenness is highest for edges connecting communities. As you
can see in this figure the edge connecting the two communities will have much
higher edge betweenness than the rest of the edges as all the shortest paths
connecting vertices of the two communities run through it.} \label{fig:edge}
\end{figure}

\subsection{Divisive Approach}
This is just the opposite of Agglomerative approach. Here we start with the
original graph and consider them in one cluster and then iteratively remove
edges based on some dissimilarity measure. The intuition is, if we remove the
edges that lie between the communities first, then we have our clusters.

\subsubsection{Edge betweenness and Community structure}
The most popular divisive hierarchical algorithm is proposed by Girvan and
Newman\cite{new2002,new2004}. These were the seminal work which ignited new
interest in the area of community detection and also opened this topic to
physicists. In these papers, Girvan and Newman focussed on the concept of
betweenness, which quantifies the participation of edges to a process. They
considered edge betweenness, which is the number of shortest paths between all
vertex pairs that run along the edge. The intuition behind this is the edges
which lies between communities will have high edge betweenness value. So
removing them will results into separation of the communities. This forms the
basis of their algorithm. Here are the steps:-
\begin{enumerate}
\item Calculate the betweenness for all edges in the network
\item Remove the edge with the highest betweenness
\item Recalculate betweenness for all edges affected by the removal
\item Repeat from step 2 until no edges remain
\end{enumerate}
The naive algorithm doesn't scale very well. It is known that the worst case
time complexity of this version is $O(n^3)$ on a sparse graph. Lot of heuristics have been
proposed to scale this algorithm, one of the first was given by Wilkins et al \cite{wilkin2004}.

\section{Partitional clustering}
If we are given a set of data points then we can apply Partitional clustering
to find clusters.  We are also give $k$ as the number of clusters we need to
find. The points are embedded in a metric space. Each vertex is represented as
a point and the distance between them is the measure of similarity between the
vertices. The main aim of such kind of algorithms is to separate the points in
$k$ clusters such as to optimize a given cost function based on distance
between points. Some of the most popular cost functions are 

\begin{itemize}
\item \textbf{Minimum k-clustering}: This represent the diameter of a cluster,
which is the largest distance between two points of a cluster. The intuition
behind this is to keep the clusters compact so that the diameter is minimized.
\item \textbf{k-clustering sum}:Same as above, but the diameter is replaced by
the average distance between all pairs of points of a cluster.
\item \textbf{k-center}:For each cluster $i$, we define a centroid and we
compute the minimum $d_i$ of the distances of each point from the centroid of
its cluster. The main aim is to choose clusters and centroids to minimize the
largest value of $d_i$. 
\item \textbf{k-media}:Same as k-center, except the distance  is replaced by
average distance.
\end{itemize}

One of the most popular Partitional algorithm is \emph{k-means clustering}
\cite{kmeans67}. The cost function in this technique is the square error
function given below. \\ \begin{equation} \sum_{i=1}^k \sum_{x_j \in S_i} ||x_j
- c_i||^2 \end{equation} where $S_i$ indicates the set of points in i-th
  cluster and $c_i$ is its centroid. This problem can be solved by using the
algorithms proposed by Lloyd \cite{loyd82}. Given data points, we need to find
$k$ clusters. The algorithm starts with choosing the $k$ centroids by some
heuristics, most of them try to maximize the distance between them.
In the first iteration, each vertex is assigned to the nearest centroid. 
Then the centroids for each cluster is estimated with the help points in the
clusters.  Then the process of assigning the vertex to nearest centroid and
reestimating centroids is repeated till convergence. The final output depends
heavily on the choice of the initial $k$ centroids. This is the main reason why
people normally run this algorithms few times with different initialization and
pick the clusters with minimum square function. The main reason why this
heuristic is so popular is due to its quick convergence property which makes
the analysis of large data sets fairly quick.

Another technique which is quite famous and is related to k-means clustering is
\emph{fuzzy k-means clustering} \cite{bez84}. This algorithm is widely used in
pattern  recognition.  The main motivation behind building this algorithm is
that a point may belong to two or more clusters at the same time. The cost
function is 

\begin{equation}
J_m = \sum_{i=1}^n \sum_{j=1}^k u_{ij}^m ||x_i-c_j||^2
\end{equation}

where $u_ij$ is the membership matrix, which measures the degree of membership
of point $i$ (with position $x_i$) in cluster $j$, $m$ is a real number greater
than 1 and $c_j$ is the center of cluster $j$
 \begin{equation} c_j =
\frac{\sum_{i=1}^n u_{ij}^m x_i}{\sum_{i=1}^n u_{ij}^m} 
\end{equation}

The matrix $u_{ij}$ is normalized such that the sum of the memberships of every
point in all clusters yields 1. The membership $u_ij$ is related to the distance of point $i$ from the
center of cluster $j$, as it is safe to assume that the larger this distance, the lower $u_{ij}$. This
can be expressed by the following relation

\begin{equation}
u_{ij}=\frac{1}{\sum_{l=1}^k(\frac{||x_i-c_j||}{||x_i-c_1||})^\frac{2}{m-l}}
\end{equation}
The cost function $J_m$ can be minimized by iterating Eq 3 and 4. Starting from
initial guesses for $u_{ij}$ and using Eq. 3 to compute the centers, which are
then used replaced in Eq 5, and so on. The process is stopped if the
corresponding elements of the membership matrix in consecutive iterations
differ from each other by less than some predefined value. This again depends
on the initial choice of the matrix $u_{ij}$.

The main disadvantage of Partitional clustering is the same as that of graph
partitioning algorithms i.e the number of clusters needs to be predefined and
also one needs to find the embedding in the metric space if it not natural for
some graphs.

\section{Spectral Clustering} 
Spectral clustering is another popular clustering technique which includes all
methods and techniques that partition the set of nodes into cluster by using
the eigenvectors of the similarity matrix of the nodes or other matrices which
are derived from it. To calculate the eigenvectors the matrix needs to be
symmetric and non-negative. Similarity matrix can be derived from the set of
points in the metric space or from a graph as well. Spectral clustering mainly
consists of a transformation of the initial set of objects into a set of points
in space, whose coordinates are elements of eigenvectors. The resulting points
are then clustered using the standard clustering techniques, like k-means
clustering. The main motivation behind this is due to the fact that the 
change of representation induced by the eigenvectors make the cluster
properties of the initial data more evident. In this way, spectral clustering
is able to separate data points into clusters which might not be very evident
before applying this approach. 

\begin{figure*}
\centering
%\begin{minipage}[htb]{0.48\linewidth}
\includegraphics[scale=0.5]{pics/spectral.eps}
\caption{This figure shows a graph with 5 nodes and its Laplacian and the
corresponding eigen values and eigen vectors} \label{fig:spectral}
%\end{minipage}
\end{figure*}

We will run through an example to show how to partition a graph using spectral
clustering technique.  As given in Figure \ref{fig:spectral}, we have a graph with
five nodes. The first step is to calculate its Laplacian. After that we
calculate the eigenvalues and corresponding eigenvectors for the Laplacian. And
then we sort the points according to the eigenvectors corresponding to second
highest eigenvalue. As we can see it is able to separate the nodes in blue from
nodes in red.

In general there are three stages in spectral clustering:-
\begin{itemize}
\item Pre-processing
\begin{itemize}
\item Construct a matrix representation of the dataset
\end{itemize}
\item Decomposition
\begin{itemize}
\item Compute eigenvalues and eigenvectors of the matrix
\item Map each point to a lower dimensional representation based on one or more
eigenvectors
\end{itemize}
\item Grouping 
\begin{itemize}
\item Assign points to two or more clusters, based on the new representation
\end{itemize}
\end{itemize}



\section{Methods based on statistical Inference}
Statistical inference aims at deducing properties of data sets, starting from a
set of observations and model hypotheses. In this section I will give a brief
overview of how to use \textbf{generative models} to find clusters in a graph.
Bayesian inference uses observations to estimate the probability that a given
hypothesis is true.  It consists of two elements: first, the evidence,
expressed by the information $D$ one has about the system and second, a
statistical model with parameters $\{\theta\}$. In general, Bayesian inference
start by writing the likelihood $P(D|\{\theta\})$ that the observed evidence is
produced by the model for a given set of parameters $\{\theta\}$. The aim is to
determine $\{\theta\}$ that maximizes the posterior distribution
$P(\{\theta\}|D)$ of the parameters given the model and the evidence. By using Baye's theorem
one has 
\begin{equation}
P(\{\theta\}|D) = \frac{1}{Z}P(D|\{\theta\}) P(\{\theta\})
\end{equation}
where $P(\{\theta\})$ is the prior distribution of the model parameters and 

\begin{equation}
Z= \int P(D|\{\theta\}) P(\{\theta\}) d\theta
\end{equation}
The most challenging part is computing the above integral and also the choice
of the prior distribution is not very obvious. Hence, most of the algorithms
related to generative models differ from each other by the choice of the model
and the way they address these two issues.

Bayesian inference is quite popular in the analysis and modelling of real
graphs. Graph clustering can be considered a specific example of inference
problem. The evidence is the graph structure and the hidden information is the
classification of vertices into groups which one wants to infer. This idea
forms the basis of several recent papers. Essentially, all of them maximizes
the likelihood $P(D|\{\theta\})$ that the model is consistent with the observed
graph structure. 

\section{Conclusion}
In this survey paper, we give a brief overview of different kinds of community
finding algorithms and also discussed the advantages and disadvantages
associated with them. The scope of this survey limits us to the given
algorithms. But there are lot of different kind of modularity optimization
based algorithms and algorithms based on spin models given by physicists and so
on.

However, by looking at the advent of research in this field, one can say that
it has grown in a rather chaotic way, without any precise direction. There is
no theoretical framework that defines what clustering algorithms are proposed
to do. Different people have different idea of what clusters should be. Most of
them are consistent with each other, but as long as there is an disagreement,
it is difficult to find the best algorithm to find the clusters. One can say,
that the large number of research papers in this field might be due to above
given fact.

There has been lot of research in this area and it is still going strong. It is quite
possible hthat the scientific community will eventually converge to a formal definition
of community. Most of the algorithms these days produce similar results, but the main
question is still open what's the best algorithm and have we found the best methods to
evaluate them?
\bibliographystyle{aaai} \bibliography{bibfile}
\end{document}
